{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.SatelliteEnvironment import *\n",
    "from libs.DRLAgents.DQN_DU import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 23 , the episode reward is 0.0\n",
      "episode: 24 , the episode reward is 0.0\n",
      "episode: 25 , the episode reward is 0.0\n",
      "episode: 26 , the episode reward is -100.0\n",
      "episode: 27 , the episode reward is 0.0\n",
      "episode: 28 , the episode reward is 0.0\n",
      "episode: 29 , the episode reward is 0.0\n",
      "episode: 30 , the episode reward is -100.0\n",
      "episode: 31 , the episode reward is 0.0\n",
      "episode: 32 , the episode reward is 0.0\n",
      "episode: 33 , the episode reward is 0.0\n",
      "episode: 34 , the episode reward is 0.0\n",
      "episode: 35 , the episode reward is 0.0\n",
      "episode: 36 , the episode reward is 0.0\n",
      "episode: 37 , the episode reward is -100.0\n",
      "episode: 38 , the episode reward is 0.0\n",
      "episode: 39 , the episode reward is -100.0\n",
      "episode: 40 , the episode reward is -100.0\n",
      "episode: 41 , the episode reward is 0.0\n",
      "episode: 42 , the episode reward is 0.0\n",
      "episode: 43 , the episode reward is 0.0\n",
      "episode: 44 , the episode reward is 9.825\n",
      "episode: 45 , the episode reward is 0.0\n",
      "episode: 46 , the episode reward is -100.0\n",
      "episode: 47 , the episode reward is 0.0\n",
      "episode: 48 , the episode reward is 0.0\n",
      "episode: 49 , the episode reward is -100.0\n",
      "episode: 50 , the episode reward is 0.0\n",
      "episode: 51 , the episode reward is 0.0\n",
      "episode: 52 , the episode reward is -100.0\n",
      "episode: 53 , the episode reward is -100.0\n",
      "episode: 54 , the episode reward is 0.0\n",
      "episode: 55 , the episode reward is 0.0\n",
      "episode: 56 , the episode reward is 0.0\n",
      "episode: 57 , the episode reward is 0.0\n",
      "episode: 58 , the episode reward is -100.0\n",
      "episode: 59 , the episode reward is -100.0\n",
      "episode: 60 , the episode reward is -100.0\n",
      "episode: 61 , the episode reward is 0.0\n",
      "episode: 62 , the episode reward is -100.0\n"
     ]
    }
   ],
   "source": [
    "env = CircularOrbit()\n",
    "\n",
    "state = env.get_state()\n",
    "\n",
    "num_states = state.shape[0]\n",
    "orbit_num  = env.orbit_num\n",
    "sat_num    = env.satellite_num\n",
    "num_action = orbit_num * sat_num\n",
    "\n",
    "model      = DQN(num_states = num_states,\n",
    "                 num_action = num_action)\n",
    "\n",
    "writer     = SummaryWriter('runs/satellite')\n",
    "episodes   = 10000\n",
    "\n",
    "for i in range(episodes):\n",
    "    env.reset()\n",
    "    # env.plot()\n",
    "    ep_reward = 0\n",
    "    ep_loss = 0 \n",
    "    while True:\n",
    "        state = env.get_state()\n",
    "        \n",
    "        action = model.choose_action(state)\n",
    "\n",
    "        reward, done, info = env.step(action) # \n",
    "        \n",
    "        next_state = env.get_state() # \n",
    "        \n",
    "        model.store_transition(state, action, reward, next_state) # \n",
    "        ep_reward += reward\n",
    "\n",
    "        if model.memory_counter >= MEMORY_CAPACITY:\n",
    "\n",
    "            loss = model.learn(network_iteration= 100,\n",
    "                                batch_size= 32,\n",
    "                                gamma=0.9)\n",
    "            ep_loss += loss\n",
    "            \n",
    "            if done:\n",
    "                print(\"episode: {} , the episode reward is {}\".format(i, round(ep_reward, 3)))\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    writer.add_scalar('Episode Reward', ep_reward, i)\n",
    "    writer.add_scalar('Episode Loss', ep_loss, i)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
